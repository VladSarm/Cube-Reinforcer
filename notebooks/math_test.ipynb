{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fcade8",
   "metadata": {},
   "source": [
    "## 0) Problem Setup and Symbols\n",
    "\n",
    "We build a compact REINFORCE toy example with the same core notation as in `README.md`.\n",
    "\n",
    "Main symbols:\n",
    "- state embedding: $x$\n",
    "- policy logits: $z$\n",
    "- action probabilities: $\\pi = \\mathrm{softmax}(z)$\n",
    "- sampled action: $a$\n",
    "- log-probability: $\\log \\pi_\\theta(a\\mid s)$\n",
    "- rewards: $r_t, r_{t+1}, \\dots$\n",
    "- discounted return: $G_t$\n",
    "\n",
    "Discounted return definition:\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k+1}.\n",
    "$$\n",
    "For the two-step demonstration used below:\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1}.\n",
    "$$\n",
    "\n",
    "REINFORCE objective form (from README):\n",
    "$$\n",
    "J(\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_t G_t \\log \\pi_\\theta(a_t\\mid s_t)\\right]\n",
    "$$\n",
    "Gradient estimator:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\sum_t G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t).\n",
    "$$\n",
    "\n",
    "The next code cell defines constants for our 2x2 toy state/action encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "224b4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 2x2 setup\n",
    "N_FACES = 6\n",
    "STICKERS_PER_FACE = 4\n",
    "STATE_SIZE = N_FACES * STICKERS_PER_FACE  # 24\n",
    "ACTION_DIM = 12\n",
    "\n",
    "ACTION_NAMES = [\n",
    "    'U+', 'U-', 'D+', 'D-', 'L+', 'L-', 'R+', 'R-', 'F+', 'F-', 'B+', 'B-'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7665",
   "metadata": {},
   "source": [
    "## 1) Build 2x2 Observation and Action One-Hot Vectors\n",
    "\n",
    "This cell constructs the RL input in the same style as the project pipeline:\n",
    "- solved 2x2 color state (length 24),\n",
    "- state one-hot matrix of shape $(24, 6)$,\n",
    "- zero action-history vector of shape $(4\\cdot 12)=48$,\n",
    "- final observation vector $\\mathrm{obs}$ of length $24\\cdot 6 + 48 = 192$.\n",
    "\n",
    "It also samples one action index and creates its one-hot representation.\n",
    "\n",
    "This is only a deterministic encoding step; no optimization yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27383af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved state (6x4):\n",
      "[[0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [2 2 2 2]\n",
      " [3 3 3 3]\n",
      " [4 4 4 4]\n",
      " [5 5 5 5]]\n",
      "\n",
      "Sampled action: 8 F+\n",
      "\n",
      "State one-hot shape: (24, 6)\n",
      "[[1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]]\n",
      "\n",
      "Action history one-hot shape: (48,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "OBS shape: (192,)\n",
      "[1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "\n",
      "Action one-hot shape: (12,)\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Solved cube state for 2x2 (flat color IDs, length 24)\n",
    "state = np.repeat(np.arange(N_FACES, dtype=np.int8), STICKERS_PER_FACE)\n",
    "print('Solved state (6x4):')\n",
    "print(state.reshape(N_FACES, STICKERS_PER_FACE))\n",
    "\n",
    "# Sample one random action (0..11)\n",
    "rng = np.random.default_rng()\n",
    "action = int(rng.integers(0, ACTION_DIM))\n",
    "print('\\nSampled action:', action, ACTION_NAMES[action])\n",
    "\n",
    "# One-hot state: (24, 6)\n",
    "state_one_hot = np.zeros((STATE_SIZE, N_FACES), dtype=np.int8)\n",
    "state_one_hot[np.arange(STATE_SIZE), state.astype(np.int64)] = 1\n",
    "\n",
    "# History one-hot for last 4 actions: start with zeros -> shape (48,)\n",
    "action_history_one_hot = np.zeros((4 * ACTION_DIM,), dtype=np.int8)\n",
    "\n",
    "# Observation vector: state_one_hot.flatten + history_one_hot -> shape (192,)\n",
    "obs = np.concatenate([state_one_hot.reshape(-1), action_history_one_hot], axis=0)\n",
    "\n",
    "# Current action one-hot: shape (12,)\n",
    "action_one_hot = np.zeros((ACTION_DIM,), dtype=np.int8)\n",
    "action_one_hot[action] = 1\n",
    "\n",
    "print('\\nState one-hot shape:', state_one_hot.shape)\n",
    "print(state_one_hot)\n",
    "print('\\nAction history one-hot shape:', action_history_one_hot.shape)\n",
    "print(action_history_one_hot)\n",
    "print('\\nOBS shape:', obs.shape)\n",
    "print(obs)\n",
    "print('\\nAction one-hot shape:', action_one_hot.shape)\n",
    "print(action_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb872e6",
   "metadata": {},
   "source": [
    "## 2) Torch Forward Pass, Action Sampling, Environment Step, Reward, and Discounted Return\n",
    "\n",
    "We now perform the stochastic policy step in PyTorch:\n",
    "$$\n",
    "h_{\\text{pre}} = xW_1 + b_1, \\quad h = \\mathrm{ELU}(h_{\\text{pre}}), \\quad z = hW_2 + b_2\n",
    "$$\n",
    "$$\n",
    "\\pi = \\mathrm{softmax}(z).\n",
    "$$\n",
    "Sampling and log-probability are computed with:\n",
    "```python\n",
    "dist = torch.distributions.Categorical(logits=logits)\n",
    "action = dist.sample()\n",
    "log_prob = dist.log_prob(action)\n",
    "```\n",
    "\n",
    "Then we apply one toy cube step, compute immediate reward $r_t$, then one more step for $r_{t+1}$, and build discounted return:\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1}.\n",
    "$$\n",
    "\n",
    "We also store initial weights/logits/log-prob for exact Torch-vs-NumPy comparison later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe6b140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_action = 4 L+\n",
      "log_prob_t(action) = -2.514844358797807\n",
      "is_solved_after_step = False\n",
      "reward_t = -1.0\n",
      "reward_t_plus_1 = -1.0\n",
      "gamma = 0.95\n",
      "G_t = reward_t + gamma * reward_t_plus_1 = -1.95\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Torch forward + sample action, apply one cube step, compute discounted return\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)  # fixed seed for reproducibility\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "in_dim = 5\n",
    "hidden_dim = 4\n",
    "action_dim = 12\n",
    "lr = 1e-2\n",
    "gamma = 0.95\n",
    "\n",
    "# Same init for torch and numpy (numpy -> torch copy)\n",
    "x_np = rng.normal(size=(in_dim,)).astype(np.float64)\n",
    "W1_init_np = rng.normal(scale=0.1, size=(in_dim, hidden_dim)).astype(np.float64)\n",
    "b1_init_np = rng.normal(scale=0.01, size=(hidden_dim,)).astype(np.float64)\n",
    "W2_init_np = rng.normal(scale=0.1, size=(hidden_dim, action_dim)).astype(np.float64)\n",
    "b2_init_np = rng.normal(scale=0.01, size=(action_dim,)).astype(np.float64)\n",
    "\n",
    "dtype = torch.float64\n",
    "linear1 = nn.Linear(in_dim, hidden_dim, bias=True, dtype=dtype)\n",
    "elu_layer = nn.ELU(alpha=1.0)\n",
    "linear2 = nn.Linear(hidden_dim, action_dim, bias=True, dtype=dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear1.weight.copy_(torch.from_numpy(W1_init_np.T))\n",
    "    linear1.bias.copy_(torch.from_numpy(b1_init_np))\n",
    "    linear2.weight.copy_(torch.from_numpy(W2_init_np.T))\n",
    "    linear2.bias.copy_(torch.from_numpy(b2_init_np))\n",
    "\n",
    "# Forward + sampling\n",
    "x_t = torch.from_numpy(x_np).to(dtype=dtype).unsqueeze(0)\n",
    "h_pre_t = linear1(x_t)\n",
    "h_t = elu_layer(h_pre_t)\n",
    "logits_t = linear2(h_t).squeeze(0)\n",
    "dist = torch.distributions.Categorical(logits=logits_t)\n",
    "action_t = dist.sample()\n",
    "log_prob_t = dist.log_prob(action_t)\n",
    "sampled_action = int(action_t.item())\n",
    "\n",
    "# Save reference tensors/values BEFORE update\n",
    "logits_before_torch_np = logits_t.detach().cpu().numpy().copy()\n",
    "log_prob_before_torch = float(log_prob_t.item())\n",
    "\n",
    "W1_before_torch_np = linear1.weight.detach().cpu().numpy().T.copy()\n",
    "b1_before_torch_np = linear1.bias.detach().cpu().numpy().copy()\n",
    "W2_before_torch_np = linear2.weight.detach().cpu().numpy().T.copy()\n",
    "b2_before_torch_np = linear2.bias.detach().cpu().numpy().copy()\n",
    "\n",
    "# Apply one action to the 2x2 state we created earlier (toy transition for notebook)\n",
    "state_after_step = np.roll(state, sampled_action + 1)\n",
    "is_solved_after_step = bool(np.array_equal(state_after_step, state))\n",
    "reward_t = -1.0 if not is_solved_after_step else 0.0\n",
    "\n",
    "# One extra lookahead step for discounted return demo\n",
    "state_after_step_2 = np.roll(state_after_step, sampled_action + 1)\n",
    "is_solved_after_step_2 = bool(np.array_equal(state_after_step_2, state))\n",
    "reward_t_plus_1 = -1.0 if not is_solved_after_step_2 else 0.0\n",
    "\n",
    "# Discounted return used by REINFORCE weight\n",
    "G_t = reward_t + gamma * reward_t_plus_1\n",
    "\n",
    "print('sampled_action =', sampled_action, ACTION_NAMES[sampled_action])\n",
    "print('log_prob_t(action) =', log_prob_before_torch)\n",
    "print('is_solved_after_step =', is_solved_after_step)\n",
    "print('reward_t =', reward_t)\n",
    "print('reward_t_plus_1 =', reward_t_plus_1)\n",
    "print('gamma =', gamma)\n",
    "print('G_t = reward_t + gamma * reward_t_plus_1 =', G_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d44707",
   "metadata": {},
   "source": [
    "## 3) Torch REINFORCE Update Step\n",
    "\n",
    "For one sampled transition, we optimize:\n",
    "$$\n",
    "\\mathcal{L} = -G_t \\, \\log \\pi_\\theta(a\\mid s),\n",
    "$$\n",
    "where $G_t$ already includes discount factor $\\gamma$.\n",
    "\n",
    "With SGD step:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta \\mathcal{L}\n",
    "= \\theta + \\eta \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s).\n",
    "$$\n",
    "So this matches the REINFORCE direction exactly.\n",
    "\n",
    "This cell performs the torch backward pass and one optimizer step, then stores final torch weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b761eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch loss = -4.903946499655723\n",
      "Torch SGD step done with lr = 0.01\n",
      "Discounted return G_t = -1.95\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Torch REINFORCE-style gradient step with discounted return G_t\n",
    "import torch\n",
    "\n",
    "# loss = - G_t * log_prob(a|s), where G_t includes gamma\n",
    "optimizer = torch.optim.SGD(list(linear1.parameters()) + list(linear2.parameters()), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "loss_t = -(G_t * log_prob_t)\n",
    "loss_t.backward()\n",
    "optimizer.step()\n",
    "\n",
    "W1_after_torch_np = linear1.weight.detach().cpu().numpy().T.copy()\n",
    "b1_after_torch_np = linear1.bias.detach().cpu().numpy().copy()\n",
    "W2_after_torch_np = linear2.weight.detach().cpu().numpy().T.copy()\n",
    "b2_after_torch_np = linear2.bias.detach().cpu().numpy().copy()\n",
    "\n",
    "print('Torch loss =', float(loss_t.item()))\n",
    "print('Torch SGD step done with lr =', lr)\n",
    "print('Discounted return G_t =', G_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87826e22",
   "metadata": {},
   "source": [
    "## 4) Manual NumPy Gradient and One-Step Update (Same Action, Same Discounted Return)\n",
    "\n",
    "Now we reproduce the same update in pure NumPy, using **the same** sampled action $a$ and the same discounted return $G_t=r_t+\\gamma r_{t+1}$.\n",
    "\n",
    "From the softmax-logprob derivative:\n",
    "$$\n",
    "\\delta = \\frac{\\partial \\log \\pi_\\theta(a\\mid s)}{\\partial z} = e_a - \\pi\n",
    "$$\n",
    "Layer gradients:\n",
    "$$\n",
    "\\frac{\\partial \\log \\pi}{\\partial W_2}=h^\\top\\delta, \\qquad\n",
    "\\frac{\\partial \\log \\pi}{\\partial b_2}=\\delta\n",
    "$$\n",
    "$$\n",
    "g_h=W_2\\delta, \\qquad g_{\\text{pre}}=g_h\\odot \\mathrm{ELU}'(h_{\\text{pre}})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\log \\pi}{\\partial W_1}=x^\\top g_{\\text{pre}}, \\qquad\n",
    "\\frac{\\partial \\log \\pi}{\\partial b_1}=g_{\\text{pre}}.\n",
    "$$\n",
    "\n",
    "Then we apply:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\eta \\, G_t \\, \\nabla_\\theta \\log \\pi_\\theta(a\\mid s),\n",
    "$$\n",
    "and compare final NumPy weights with final Torch weights element-wise.\n",
    "\n",
    "If everything is consistent, differences should be near numerical precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24e091ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-update comparison:\n",
      "max|logits_torch - logits_numpy| = 2.0816681711721685e-17\n",
      "|log_prob_torch - log_prob_numpy| = 4.440892098500626e-16\n",
      "\n",
      "Post-update weight comparison:\n",
      "max|W1_torch - W1_numpy| = 1.3877787807814457e-17\n",
      "max|b1_torch - b1_numpy| = 1.734723475976807e-18\n",
      "max|W2_torch - W2_numpy| = 4.336808689942018e-19\n",
      "max|b2_torch - b2_numpy| = 3.469446951953614e-18\n",
      "\n",
      "All close?\n",
      "W1 close: True\n",
      "b1 close: True\n",
      "W2 close: True\n",
      "b2 close: True\n"
     ]
    }
   ],
   "source": [
    "# Step 3: NumPy mirror with the SAME action and discounted return, then compare final weights\n",
    "import numpy as np\n",
    "\n",
    "def linear(x, W, b):\n",
    "    return x @ W + b\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0.0, x, alpha * (np.exp(x) - 1.0))\n",
    "\n",
    "def elu_prime(x, alpha=1.0):\n",
    "    return np.where(x > 0.0, 1.0, alpha * np.exp(x))\n",
    "\n",
    "def softmax(z):\n",
    "    z_shifted = z - np.max(z)\n",
    "    e = np.exp(z_shifted)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def one_hot(i, n):\n",
    "    v = np.zeros((n,), dtype=np.float64)\n",
    "    v[int(i)] = 1.0\n",
    "    return v\n",
    "\n",
    "# Start from exactly the same initial parameters\n",
    "W1_np = W1_init_np.copy()\n",
    "b1_np = b1_init_np.copy()\n",
    "W2_np = W2_init_np.copy()\n",
    "b2_np = b2_init_np.copy()\n",
    "\n",
    "# Forward (NumPy)\n",
    "h_pre_np = linear(x_np, W1_np, b1_np)\n",
    "h_np = elu(h_pre_np)\n",
    "logits_np = linear(h_np, W2_np, b2_np)\n",
    "pi_np = softmax(logits_np)\n",
    "\n",
    "# Same sampled action from torch\n",
    "a = sampled_action\n",
    "log_prob_np = float(np.log(pi_np[a]))\n",
    "\n",
    "# grad log pi(a|s)\n",
    "delta = one_hot(a, action_dim) - pi_np\n",
    "dW2_logp = np.outer(h_np, delta)\n",
    "db2_logp = delta.copy()\n",
    "g_h = W2_np @ delta\n",
    "g_pre = g_h * elu_prime(h_pre_np)\n",
    "dW1_logp = np.outer(x_np, g_pre)\n",
    "db1_logp = g_pre.copy()\n",
    "\n",
    "# REINFORCE update: theta += lr * G_t * grad_log_prob, where G_t uses gamma\n",
    "W2_np = W2_np + lr * G_t * dW2_logp\n",
    "b2_np = b2_np + lr * G_t * db2_logp\n",
    "W1_np = W1_np + lr * G_t * dW1_logp\n",
    "b1_np = b1_np + lr * G_t * db1_logp\n",
    "\n",
    "print('Pre-update comparison:')\n",
    "print('max|logits_torch - logits_numpy| =', float(np.max(np.abs(logits_before_torch_np - logits_np))))\n",
    "print('|log_prob_torch - log_prob_numpy| =', float(abs(log_prob_before_torch - log_prob_np)))\n",
    "\n",
    "print('\\nPost-update weight comparison:')\n",
    "print('max|W1_torch - W1_numpy| =', float(np.max(np.abs(W1_after_torch_np - W1_np))))\n",
    "print('max|b1_torch - b1_numpy| =', float(np.max(np.abs(b1_after_torch_np - b1_np))))\n",
    "print('max|W2_torch - W2_numpy| =', float(np.max(np.abs(W2_after_torch_np - W2_np))))\n",
    "print('max|b2_torch - b2_numpy| =', float(np.max(np.abs(b2_after_torch_np - b2_np))))\n",
    "\n",
    "tol = 1e-10\n",
    "print('\\nAll close?')\n",
    "print('W1 close:', np.allclose(W1_after_torch_np, W1_np, atol=tol, rtol=0.0))\n",
    "print('b1 close:', np.allclose(b1_after_torch_np, b1_np, atol=tol, rtol=0.0))\n",
    "print('W2 close:', np.allclose(W2_after_torch_np, W2_np, atol=tol, rtol=0.0))\n",
    "print('b2 close:', np.allclose(b2_after_torch_np, b2_np, atol=tol, rtol=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf596cc",
   "metadata": {},
   "source": [
    "## 5) Batch REINFORCE Example (Exactly as in `trainer.py`)\n",
    "\n",
    "Here we reproduce the trainer batch logic with **two parallel trajectories**:\n",
    "- Env 0: terminates after **1** step\n",
    "- Env 1: terminates after **2** steps\n",
    "\n",
    "We use tensors with shape `[T, B]` and the same formulas as in the trainer:\n",
    "\n",
    "- discounted returns (backward over time):\n",
    "$$\n",
    "R_t = r_t + \\gamma R_{t+1}\n",
    "$$\n",
    "- masked policy loss:\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{\\sum_{t,b} \\log \\pi(a_{t,b}|s_{t,b})\\,R_{t,b}\\,m_{t,b}}{\\sum_{t,b} m_{t,b}}\n",
    "$$\n",
    "where `m_{t,b} = active_mask[t,b]` and inactive timesteps do not contribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54b303cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0.9\n",
      "rewards_total [T,B]:\n",
      " tensor([[-1., -1.],\n",
      "        [ 0., -1.]])\n",
      "active_mask [T,B]:\n",
      " tensor([[1., 1.],\n",
      "        [0., 1.]])\n",
      "log_probs [T,B]:\n",
      " tensor([[-0.2000, -0.5000],\n",
      "        [-9.9900, -0.7000]])\n",
      "returns [T,B]:\n",
      " tensor([[-1.0000, -1.9000],\n",
      "        [ 0.0000, -1.0000]])\n",
      "weighted_terms = log_probs * returns * active_mask:\n",
      " tensor([[0.2000, 0.9500],\n",
      "        [-0.0000, 0.7000]])\n",
      "valid_count = 3.0\n",
      "loss = -0.6166666150093079\n",
      "\n",
      "Per-env active lengths:\n",
      "env0 active steps = 1\n",
      "env1 active steps = 2\n",
      "\n",
      "Contributions used in loss:\n",
      "t=0, env=0, logp=-0.200, return=-1.000, product=0.200\n",
      "t=0, env=1, logp=-0.500, return=-1.900, product=0.950\n",
      "t=1, env=1, logp=-0.700, return=-1.000, product=0.700\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Minimal torch batch demo with masks and loss (same as trainer.py)\n",
    "import torch\n",
    "\n",
    "# Two trajectories in a batch: B=2, max horizon T=2\n",
    "# Env0 length=1, Env1 length=2\n",
    "T, B = 2, 2\n",
    "gamma_batch = 0.9\n",
    "\n",
    "# reward_total[t, b]\n",
    "# t=0: both envs active and take a step -> -1 each\n",
    "# t=1: env0 already done (inactive, reward 0), env1 still active -> -1\n",
    "rewards_total = torch.tensor([\n",
    "    [-1.0, -1.0],\n",
    "    [ 0.0, -1.0],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# active_mask[t, b] exactly as trainer uses it\n",
    "# Env0: [1, 0], Env1: [1, 1]\n",
    "active_mask = torch.tensor([\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 1.0],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Example log-probabilities for sampled actions at each [t, b]\n",
    "# Value at inactive step (t=1,b=0) is arbitrary and must be ignored by mask.\n",
    "log_probs = torch.tensor([\n",
    "    [-0.20, -0.50],\n",
    "    [-9.99, -0.70],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# 1) Discounted returns per env over time (same backward loop as trainer.py)\n",
    "returns = torch.zeros_like(rewards_total)\n",
    "running = torch.zeros((B,), dtype=torch.float32)\n",
    "for t in range(T - 1, -1, -1):\n",
    "    running = rewards_total[t] + gamma_batch * running\n",
    "    returns[t] = running\n",
    "\n",
    "# 2) Masked loss (exact trainer formula)\n",
    "valid_count = torch.clamp(active_mask.sum(), min=1.0)\n",
    "weighted_terms = log_probs * returns * active_mask\n",
    "loss = -(weighted_terms.sum() / valid_count)\n",
    "\n",
    "print('gamma =', gamma_batch)\n",
    "print('rewards_total [T,B]:\\n', rewards_total)\n",
    "print('active_mask [T,B]:\\n', active_mask)\n",
    "print('log_probs [T,B]:\\n', log_probs)\n",
    "print('returns [T,B]:\\n', returns)\n",
    "print('weighted_terms = log_probs * returns * active_mask:\\n', weighted_terms)\n",
    "print('valid_count =', float(valid_count.item()))\n",
    "print('loss =', float(loss.item()))\n",
    "\n",
    "# Sanity checks by trajectory\n",
    "print('\\nPer-env active lengths:')\n",
    "print('env0 active steps =', int(active_mask[:, 0].sum().item()))\n",
    "print('env1 active steps =', int(active_mask[:, 1].sum().item()))\n",
    "\n",
    "print('\\nContributions used in loss:')\n",
    "for t in range(T):\n",
    "    for b in range(B):\n",
    "        if active_mask[t, b] > 0:\n",
    "            print(\n",
    "                f't={t}, env={b}, logp={log_probs[t,b].item():.3f}, '\n",
    "                f'return={returns[t,b].item():.3f}, product={weighted_terms[t,b].item():.3f}'\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cube-reinforcer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
